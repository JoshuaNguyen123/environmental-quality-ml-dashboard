supervised:
  linear_regression:
    type: regression
    params: {}

  random_forest_reg:
    type: regression
    params:
      n_estimators: 200
      max_depth: 15
      min_samples_split: 5
      min_samples_leaf: 2
      random_state: 42
      n_jobs: -1

  extra_trees_reg:
    type: regression
    params:
      n_estimators: 300
      max_depth: 16
      min_samples_split: 4
      min_samples_leaf: 2
      random_state: 42
      n_jobs: -1

  hist_gradient_boosting_reg:
    type: regression
    params:
      max_depth: 8
      learning_rate: 0.08
      max_iter: 300
      min_samples_leaf: 25
      random_state: 42

  logistic_reg:
    type: classification
    params:
      C: 1.0
      max_iter: 1000
      random_state: 42
      solver: lbfgs

  random_forest_clf:
    type: classification
    params:
      n_estimators: 200
      max_depth: 15
      min_samples_split: 5
      min_samples_leaf: 2
      random_state: 42
      n_jobs: -1

  extra_trees_clf:
    type: classification
    params:
      n_estimators: 300
      max_depth: 16
      min_samples_split: 4
      min_samples_leaf: 2
      random_state: 42
      n_jobs: -1

  hist_gradient_boosting_clf:
    type: classification
    params:
      max_depth: 8
      learning_rate: 0.08
      max_iter: 300
      min_samples_leaf: 25
      random_state: 42

unsupervised:
  kmeans:
    params:
      n_clusters: 4
      n_init: 10
      max_iter: 300
      random_state: 42

  gmm:
    params:
      n_components: 4
      covariance_type: full
      n_init: 5
      random_state: 42

  agglomerative:
    params:
      n_clusters: 4
      linkage: ward

  dbscan:
    params:
      eps: 0.85
      min_samples: 18

deep_learning:
  mlp:
    hidden_layers: [128, 64]
    activation: relu
    learning_rate: 0.001
    learning_rate_schedule: constant
    alpha: 0.0001
    epochs: 100
    batch_size: 64
    early_stopping_patience: 10
    dropout: 0.2
    random_state: 42

  mlp_deep:
    hidden_layers: [256, 128, 64]
    activation: relu
    learning_rate: 0.0008
    learning_rate_schedule: adaptive
    alpha: 0.0002
    epochs: 140
    batch_size: 64
    early_stopping_patience: 12
    dropout: 0.2
    random_state: 42

  mlp_regularized:
    hidden_layers: [128, 64, 32]
    activation: tanh
    learning_rate: 0.0007
    learning_rate_schedule: adaptive
    alpha: 0.001
    epochs: 120
    batch_size: 64
    early_stopping_patience: 14
    dropout: 0.2
    random_state: 42

splits:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
